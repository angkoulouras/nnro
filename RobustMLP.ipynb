{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic data parameters\n",
    "num_samples = int(1e6)\n",
    "num_features = 500\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "# Generate coefficients for the logistic regression function\n",
    "coefficients = np.random.uniform(low=-5, high=5, size=(num_features))\n",
    "\n",
    "# Generate synthetic data\n",
    "# X_synthetic = np.random.uniform(low=-3, high=3, size=(num_samples, num_features))\n",
    "X_synthetic = np.random.normal(loc=0, scale=1, size=(num_samples, num_features))\n",
    "\n",
    "\n",
    "# Generate labels using a logistic regression function\n",
    "logits = np.dot(X_synthetic, coefficients)\n",
    "probabilities = 1 / (1 + np.exp(-logits))\n",
    "y_synthetic = np.round(probabilities).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test_all = scaler.transform(X_test)\n",
    "y_test_all = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The abstract robust model\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RobustifyNetwork(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes, epsilon):\n",
    "        super(RobustifyNetwork, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.train_variables = []\n",
    "        pass\n",
    "\n",
    "    def feedforward_pass(self, input):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input):\n",
    "        self.x_input = input\n",
    "\n",
    "        return self.feedforward_pass(input)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input, label, epsilon, robust=True, type_robust='linf',):\n",
    "        self._full_call(input, label, epsilon, robust=robust,\n",
    "                        evaluate=False, type_robust=type_robust)\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate(self, input, label, epsilon, step=-1, summary=None, type_robust='linf', evaluate_bound=False):\n",
    "        self._full_call(input, label, epsilon, step=step, robust=True,\n",
    "                        evaluate=True, summary=summary, type_robust=type_robust)\n",
    "\n",
    "    def evaluate_bound(self, input, label, epsilon):\n",
    "        self._full_call(input, label, epsilon,  robust=True,  type_robust='certificate',\n",
    "                      evaluate=True)\n",
    "\n",
    "    def evaluate_approx_bound(self, input, label, epsilon, type_robust):\n",
    "        self._full_call(input, label, epsilon,  robust=True,  type_robust=type_robust,\n",
    "                      evaluate=True, evaluate_bound=True)\n",
    "\n",
    "    def _full_call(self, input, label, epsilon, robust=True, evaluate=False, evaluate_bound=False,\n",
    "                 summary=None, step=-1, type_robust='linf'):\n",
    "\n",
    "        self.x_input = input\n",
    "        self.y_input = label\n",
    "\n",
    "        if not robust: #vanilla training\n",
    "\n",
    "            print(\"Not robust\")\n",
    "\n",
    "            with tf.GradientTape() as self.tape:\n",
    "\n",
    "                self.feedforward_pass(self.x_input)\n",
    "\n",
    "                y_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                      labels=tf.cast(self.y_input, tf.int32), logits=self.pre_softmax)\n",
    "                self.loss = tf.reduce_mean(y_xent)\n",
    "\n",
    "        else: #robust training\n",
    "\n",
    "            if type_robust == \"clipping\":\n",
    "                self.M = tf.math.minimum(1 - self.x_input, epsilon)\n",
    "                self.m = tf.math.maximum(-self.x_input, -epsilon)\n",
    "\n",
    "            if not type_robust == \"certificate\":\n",
    "\n",
    "                with tf.GradientTape() as self.tape:\n",
    "                    with tf.GradientTape(persistent=True) as self.second_tape:\n",
    "\n",
    "                        self.second_tape.watch(self.x_input)\n",
    "                        self.feedforward_pass(self.x_input)\n",
    "\n",
    "                        if type_robust == 'grad': #linf\n",
    "                            y_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                labels=self.y_input, logits=self.pre_softmax)\n",
    "                            self.xent = tf.reduce_mean(y_xent)\n",
    "                            self.loss = self.xent + epsilon*self.second_tape.gradient(self.xent, self.x_input)\n",
    "\n",
    "                        else:\n",
    "                            # Compute linear approximation for robust cross-entropy.\n",
    "                            data_range = tf.range(tf.shape(self.y_input)[0])\n",
    "                            indices = tf.map_fn(lambda n: tf.stack([n, tf.cast(self.y_input[n], tf.int32)]), data_range)\n",
    "\n",
    "                            self.nom_exponent = []\n",
    "                            for i in range(self.num_classes):\n",
    "                                self.nom_exponent += [self.pre_softmax[:,i] - tf.gather_nd(self.pre_softmax, indices)]\n",
    "\n",
    "                            sum_exps = 0\n",
    "                            for i in range(self.num_classes):\n",
    "                                grad = self.second_tape.gradient(self.nom_exponent[i], self.x_input)\n",
    "                                if type_robust == 'clipping': #linf\n",
    "                                    positive_terms = tf.math.multiply(self.M, tf.nn.relu(grad[0]))\n",
    "                                    negative_terms = tf.math.multiply(self.m, tf.nn.relu(-grad[0]))\n",
    "                                    exponent = tf.reduce_sum(positive_terms - negative_terms, axis=1) + self.nom_exponent[i]\n",
    "                                elif type_robust == 'l1':\n",
    "                                    exponent = np.sqrt(self.num_features) * epsilon * tf.reduce_max(tf.abs(grad), axis=1) + \\\n",
    "                                               self.nom_exponent[i]\n",
    "                                elif type_robust == 'l1+inf':\n",
    "                                    # exponent = np.sqrt(self.num_features) * epsilon * tf.reduce_max(tf.abs(grad), axis=1) + \\\n",
    "                                    #          epsilon * tf.reduce_sum(tf.abs(grad), axis=1) + self.nom_exponent[i]\n",
    "                                    # exponent = tf.sqrt(tf.convert_to_tensor(self.num_features, dtype=tf.float32)) * epsilon * tf.reduce_max(tf.abs(grad), axis=1) + \\\n",
    "                                    #  epsilon * tf.reduce_sum(tf.abs(grad), axis=1) + self.nom_exponent[i]\n",
    "                                    # exponent = tf.sqrt(tf.cast(self.num_features, dtype=tf.float32)) * epsilon * tf.reduce_max(tf.abs(grad), axis=1) + epsilon * tf.reduce_sum(tf.abs(grad), axis=1) + self.nom_exponent[i]\n",
    "                                    exponent = tf.sqrt(tf.cast(self.num_features, dtype=tf.float32)) * epsilon * tf.reduce_max(tf.abs(grad), axis=1) + epsilon * tf.reduce_sum(tf.square(grad), axis=1) + self.nom_exponent[i]\n",
    "                                    # exponent = epsilon * tf.reduce_sum(tf.square(grad), axis=1) + self.nom_exponent[i]\n",
    "\n",
    "                                else: #linf\n",
    "                                    exponent = epsilon * tf.reduce_sum(tf.abs(grad), axis=1) + self.nom_exponent[i]\n",
    "                                sum_exps += tf.math.exp(exponent)\n",
    "\n",
    "                            self.loss = tf.reduce_mean(tf.math.log(sum_exps))\n",
    "\n",
    "            else: #certificate objective:\n",
    "                if not evaluate:\n",
    "                    with tf.GradientTape() as self.tape:\n",
    "\n",
    "                        self.feedforward_pass(self.x_input)\n",
    "\n",
    "                        #L1 certificate -- epsilon is converted\n",
    "                        self.loss, self.acc_bound = self.certificate_loss(np.sqrt(self.num_features) * epsilon, label)\n",
    "\n",
    "                else:\n",
    "                    self.feedforward_pass(self.x_input)\n",
    "\n",
    "                    self.loss, self.acc_bound = self.certificate_loss(epsilon, label)\n",
    "\n",
    "                    self.acc_bound = (self.acc_bound).numpy()\n",
    "\n",
    "        if not evaluate:\n",
    "\n",
    "            self.optimizer.apply_gradients(zip(self.tape.gradient(self.loss, self.train_variables),\n",
    "                                               self.train_variables))\n",
    "            #print(\"\\n Graph Created! \\n\")\n",
    "\n",
    "        else:\n",
    "            # Evaluation\n",
    "            y_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=tf.cast(label, tf.int32), logits=self.pre_softmax)\n",
    "            self.xent = tf.reduce_mean(y_xent)\n",
    "\n",
    "            self.y_pred = tf.argmax(self.pre_softmax, 1)\n",
    "            correct_prediction = tf.equal(tf.cast(self.y_pred, tf.int64), label)\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(correct_prediction, tf.int64))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            if evaluate_bound==True:\n",
    "                self.eval_approx_bound = (self.loss).numpy()\n",
    "                self.eval_xent = (self.xent).numpy()\n",
    "\n",
    "            if summary:\n",
    "                with summary.as_default():\n",
    "                    tf.summary.scalar('Cross Entropy', self.xent, step)\n",
    "                    tf.summary.scalar('Accuracy', self.accuracy, step)\n",
    "                    tf.summary.scalar('Robust Loss', self.loss, step)\n",
    "                    tf.summary.scalar('Learning Rate', self.optimizer.learning_rate(step), step)\n",
    "                    tf.summary.scalar('Epsilon', epsilon, step)\n",
    "\n",
    "            #print(\"\\n Evaluate Graph Created! \\n\")\n",
    "\n",
    "    def load_all(self, path, load_optimizer=True):\n",
    "\n",
    "        if load_optimizer:\n",
    "            opt_weights = np.load(path + '_optimizer.npy', allow_pickle=True)\n",
    "\n",
    "            grad_vars = self.trainable_weights\n",
    "            zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
    "            self.optimizer.apply_gradients(zip(zero_grads, grad_vars))\n",
    "            self.optimizer.set_weights(opt_weights)\n",
    "\n",
    "        self.load_weights(path)\n",
    "\n",
    "    def save_all(self, path):\n",
    "        self.save_weights(path)\n",
    "        np.save(path + '_optimizer.npy', self.optimizer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class robustMLP(RobustifyNetwork):\n",
    "    def __init__(self, config, num_features):\n",
    "        super().__init__(config['num_classes'], config['epsilon'])\n",
    "\n",
    "        l1_size = config['l1_size']\n",
    "        l2_size = config['l2_size']\n",
    "        l3_size = config['l3_size']\n",
    "        l4_size = config['l4_size']\n",
    "\n",
    "        initial_learning_rate = float(config['initial_learning_rate'])\n",
    "        training_batch_size = config['training_batch_size']\n",
    "        num_classes = config['num_classes']\n",
    "        batch_decrease_learning_rate = float(config['batch_decrease_learning_rate'])\n",
    "\n",
    "        self.mode = 'train'\n",
    "        self.num_features = num_features\n",
    "        self.train_variables = []\n",
    "\n",
    "        self.W1 = self._weight_variable([num_features, l1_size])\n",
    "        self.train_variables += [self.W1]\n",
    "        self.b1 = self._bias_variable([l1_size])\n",
    "        self.train_variables += [self.b1]\n",
    "\n",
    "        self.W2 = self._weight_variable([l1_size, l2_size])\n",
    "        self.train_variables += [self.W2]\n",
    "        self.b2 = self._bias_variable([l2_size])\n",
    "        self.train_variables += [self.b2]\n",
    "\n",
    "        self.W3 = self._weight_variable([l2_size, l3_size])\n",
    "        self.train_variables += [self.W3]\n",
    "        self.b3 = self._bias_variable([l3_size])\n",
    "        self.train_variables += [self.b3]\n",
    "\n",
    "        self.W4 = self._weight_variable([l3_size, l4_size])\n",
    "        self.train_variables += [self.W4]\n",
    "        self.b4 = self._bias_variable([l4_size])\n",
    "        self.train_variables += [self.b4]\n",
    "\n",
    "        self.W5 = self._weight_variable([l4_size, num_classes])\n",
    "        self.train_variables += [self.W5]\n",
    "        self.b5 = self._bias_variable([num_classes])\n",
    "        self.train_variables += [self.b5]\n",
    "\n",
    "        # Setting up the optimizer\n",
    "        self.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate,\n",
    "            training_batch_size * batch_decrease_learning_rate,\n",
    "            0.85,\n",
    "            staircase=True\n",
    "        )\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    def feedforward_pass(self, input):\n",
    "        # Fully connected layers.\n",
    "        self.h1 = tf.nn.relu(tf.matmul(tf.cast(input, dtype=tf.float32), self.W1) + self.b1)\n",
    "        self.h2 = tf.nn.relu(tf.matmul(self.h1, self.W2) + self.b2)\n",
    "        self.h3 = tf.nn.relu(tf.matmul(self.h2, self.W3) + self.b3)\n",
    "        self.h4 = tf.nn.relu(tf.matmul(self.h3, self.W4) + self.b4)\n",
    "        self.pre_softmax = tf.matmul(self.h4, self.W5) + self.b5\n",
    "        return self.pre_softmax\n",
    "\n",
    "    def set_mode(self, mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_variable(shape):\n",
    "        initial = tf.keras.initializers.GlorotUniform()\n",
    "        return tf.Variable(initial_value=initial(shape), name=str(np.random.randint(1e10)), trainable=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "  num_runs = 5\n",
    "\n",
    "  hypothesis_stability_res = []\n",
    "  uniform_stability_res = []\n",
    "  error_stability_res = []\n",
    "\n",
    "  for i in range(num_runs):\n",
    "\n",
    "    num_samples = 250\n",
    "    X_synthetic = np.random.normal(loc=0, scale=1, size=(num_samples, num_features))\n",
    "    logits = np.dot(X_synthetic, coefficients)\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    y_synthetic = np.round(probabilities).astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    rhos = [0.00, 0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 1.0, 1.5]\n",
    "    # rhos = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "    # rhos = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "    rhos = np.logspace(-4, 0.3, 10)\n",
    "    rhos = rhos.astype(np.float32)\n",
    "    # rhos = rhos[0:3]\n",
    "    # rhos = [1.5]\n",
    "    Ws = []\n",
    "    hypothesis_stability_rhos = []\n",
    "    uniform_stability_rhos = []\n",
    "    error_stability_rhos = []\n",
    "\n",
    "    num_folds = 10\n",
    "\n",
    "    # Split the training data into 10 folds\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "\n",
    "    for rho in rhos:\n",
    "\n",
    "      config = {\n",
    "          'num_classes': 2,\n",
    "          'epsilon': rho,\n",
    "          'l1_size': 16,\n",
    "          'l2_size': 8,\n",
    "          'l3_size': 8,\n",
    "          'l4_size': 4,\n",
    "          'initial_learning_rate': 0.01,\n",
    "          'training_batch_size': 64,\n",
    "          'batch_decrease_learning_rate': 1\n",
    "      }\n",
    "\n",
    "      num_features = num_features  # Replace with the actual number of features\n",
    "      model_all = robustMLP(config, num_features)\n",
    "\n",
    "      num_epochs = 10\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        for i in range(0, len(X_train), config['training_batch_size']):\n",
    "            inputs = X_train[i:i + config['training_batch_size']]\n",
    "            labels = y_train[i:i + config['training_batch_size']]\n",
    "\n",
    "            inputs = inputs.astype(np.float32)\n",
    "            labels = labels.astype(np.float32)\n",
    "\n",
    "            # Train with 'l1+inf' loss\n",
    "            model_all.train_step(inputs, labels, epsilon=config['epsilon'], type_robust='l1+inf')\n",
    "            # model_all.train_step(inputs, labels, epsilon=config['epsilon'], robust=False)\n",
    "\n",
    "        # # Evaluate on the test data after each epoch\n",
    "        # train_predictions = tf.nn.softmax(model_all.feedforward_pass(X_train.astype(np.float32)), axis=None, name=None)\n",
    "        # train_accuracy = accuracy_score(y_train, np.argmax(train_predictions, axis=1))\n",
    "        # test_predictions = tf.nn.softmax(model_all.feedforward_pass(X_test.astype(np.float32)), axis=None, name=None)\n",
    "        # test_accuracy = accuracy_score(y_test, np.argmax(test_predictions, axis=1))\n",
    "\n",
    "        # # Assuming binary classification for AUC calculation\n",
    "        # train_auc = roc_auc_score(y_train, train_predictions[:, 1])\n",
    "        # test_auc = roc_auc_score(y_test, test_predictions[:, 1])\n",
    "\n",
    "      W = np.array(model_all.get_weights(), dtype=object)\n",
    "      Ws.append(sum(sum(abs(W[0]))) + sum(abs(W[1])))\n",
    "\n",
    "      # Create an array to store the weights of each model\n",
    "      all_model_weights = []\n",
    "\n",
    "      for fold, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "\n",
    "          # Get the training and test data for this fold\n",
    "          X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
    "          y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "          # Create an instance of your robustOneLayer model for each fold\n",
    "          model = robustMLP(config, num_features)\n",
    "\n",
    "          for epoch in range(num_epochs):\n",
    "            for i in range(0, len(X_train), config['training_batch_size']):\n",
    "                inputs = X_train[i:i + config['training_batch_size']]\n",
    "                labels = y_train[i:i + config['training_batch_size']]\n",
    "\n",
    "                inputs = inputs.astype(np.float32)\n",
    "                labels = labels.astype(np.float32)\n",
    "\n",
    "                # Train with 'l1+inf' loss\n",
    "                model.train_step(inputs, labels, epsilon=config['epsilon'], type_robust='l1+inf')\n",
    "                # model.train_step(inputs, labels, epsilon=config['epsilon'], robust=False)\n",
    "\n",
    "          # Save the weights of the model after each fold\n",
    "          all_model_weights.append(np.array(model.get_weights(), dtype=object))\n",
    "\n",
    "      hypothesis_stability_all = []\n",
    "      uniform_stability_all = []\n",
    "      error_stability_all = []\n",
    "\n",
    "      for fold2_weights in all_model_weights:  # Skip the first fold since it's already used for model_fold1\n",
    "\n",
    "          # Calculate the loss at each point in the test set separately\n",
    "          max_loss_diff = 0.0\n",
    "\n",
    "          # Create a new instance of the model_fold2 for each fold\n",
    "          model_fold2 = robustMLP(config, num_features)\n",
    "          model_fold2.set_weights(fold2_weights)\n",
    "\n",
    "          # Model 1\n",
    "          logits_test_fold1 = tf.nn.softmax(model_all.feedforward_pass(X_test_all))  # Note: Pass the entire test set as a batch\n",
    "          loss_fold1 = tf.keras.losses.sparse_categorical_crossentropy(y_test_all, logits_test_fold1)\n",
    "\n",
    "          # Model 2\n",
    "          logits_test_fold2 = tf.nn.softmax(model_fold2.feedforward_pass(X_test_all))\n",
    "          loss_fold2 = tf.keras.losses.sparse_categorical_crossentropy(y_test_all, logits_test_fold2)\n",
    "\n",
    "          # Calculate the absolute difference in loss\n",
    "          loss_diff_at_point = np.abs(loss_fold1.numpy() - loss_fold2.numpy())\n",
    "\n",
    "          # Update the maximum absolute difference\n",
    "          hypothesis_stability = np.mean(loss_diff_at_point)\n",
    "          uniform_stability = np.max(loss_diff_at_point)\n",
    "          error_stability = np.abs(np.mean(loss_fold1.numpy()) - np.mean(loss_fold2.numpy()))\n",
    "          error_stability_all.append(error_stability)\n",
    "          hypothesis_stability_all.append(hypothesis_stability)\n",
    "          uniform_stability_all.append(uniform_stability)\n",
    "\n",
    "          # print(f\"Maximum Absolute Difference in Cross-Entropy Loss at Each Point: {max_loss_diff2:.2f}\")\n",
    "\n",
    "      hypothesis_stability_rhos.append(np.max(hypothesis_stability_all))\n",
    "      uniform_stability_rhos.append(np.max(uniform_stability_all))\n",
    "      error_stability_rhos.append(np.max(error_stability_all))\n",
    "\n",
    "    hypothesis_stability_res.append(hypothesis_stability_rhos)\n",
    "    uniform_stability_res.append(uniform_stability_rhos)\n",
    "    error_stability_res.append(error_stability_rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = 'hypothesis_stability_5_layer_2.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write each inner array as a row in the CSV file\n",
    "    for row in hypothesis_stability_res:\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f'The data has been successfully written to {csv_file_path}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
